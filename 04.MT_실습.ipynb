{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MT_정답.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVbkYvQ47ayQ"
      },
      "source": [
        "**Sequence to Sequence 네트워크와 Attention을 이용한 Machine Translation 모델 구현**\n",
        "\n",
        "- 한국어를 영어로 번역하도록 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2cxUfvGzpcZ"
      },
      "source": [
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "seed = 21\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xQ4C8wO5QcV"
      },
      "source": [
        "언어별 정보 저장을 위한 클래스 생성\n",
        "\n",
        "- 단어->아이디, 아이디->단어 사전 생성 (word2idx)\n",
        "- addSentence 함수는 문장을 띄어쓰기를 기준으로 토큰화\n",
        "- addWord 함수\n",
        "    - word2idx, idx2word 만들기\n",
        "    - 단어별 출현 횟수 구하기\n",
        "    - 전체 단어의 개수 구하기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5V2jJCG0Qot"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "UNK_token = 2\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        ## 단어->아이디, 단어별 개수 사전 선언\n",
        "\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2:\"UNK\"}\n",
        "        self.n_words = 3  # Count SOS and EOS and UNK\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        # sentence를 띄어쓰기 단위로 토큰화 하고, 토큰은 addWord 함수를 이용해서 정보 저장\n",
        "\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        # word2idx, word2count, idx2word, nwords 정보 저장\n",
        "\n",
        "        ####################  빈칸  ####################\n",
        "        \n",
        "            \n",
        "        ###############################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp3YN2_M7FSy"
      },
      "source": [
        "- 병렬 코퍼스 준비 : 영어-한국어\n",
        "- 영어는 normalize를 진행\n",
        "- readLangs 함수\n",
        "    - 파일을 읽고 쌍으로 분리\n",
        "    - Lang class를 이용하여 각 언어 정보 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1N6GRxS5d5-"
      },
      "source": [
        "def normalizeString(s):\n",
        "    s = s.lower().strip()\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "def readLangs(lang1, lang2):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # git clone을 통해 데이터를 다운로드 받는 경우, 파일 경로 수정 필요\n",
        "    f = open('%s-%s.txt' % (lang1, lang2), encoding='utf-8')\n",
        "    lines = f.readlines()\n",
        "    \n",
        "    pairs = []\n",
        "    for l in lines:\n",
        "        line = l.split(\"\\t\")\n",
        "        # 한국어, 영어\n",
        "        pair = [line[1], normalizeString(line[0])]\n",
        "        pairs.append(pair)\n",
        "\n",
        "    input_lang = Lang(lang2)\n",
        "    output_lang = Lang(lang1)\n",
        "    f.close()\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voejo56ravk2"
      },
      "source": [
        "- MAX_LENGTH : 학습에 사용할 최대문장길이\n",
        "- eng_prefixes : 해당 단어들로 시작하는 문장만 번역에 포함\n",
        "\n",
        "- filterPair : 위 조건에 만족하는 pair만 반환\n",
        "- filterPairs : 필터링된 pair만 저장해서 반환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jqvzt5j5on-"
      },
      "source": [
        "MAX_LENGTH = 30\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "    ####################  빈칸  ####################\n",
        "\n",
        "    # 영어는 해당하는 구문으로 시작하고 영어와 한국어 둘다 max_length보다 작은 데이터일 경우에만 True 반환\n",
        "    \n",
        "    ###############################################\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    # filterPair가 True인 데이터만 저장후 반환\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00R1vJMkbeg5"
      },
      "source": [
        "데이터 준비 및 결과 확인\n",
        "- **학습과 검증에 필요한 데이터쌍을 구분**\n",
        "- 이전 단계에서 만든 클래스와 함수를 통해 결과 확인\n",
        "- 전체 문장 개수(pair 개수)\n",
        "- 필터링 후 문장개수(pair 개수)\n",
        "- 각 언어별로, 사전에 등록된 단어 개수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b318yrC5vEL"
      },
      "source": [
        "def prepareData(lang1, lang2):\n",
        "\n",
        "    # 이전에 만든 클래스를 활용해서 영어 한국어의 정보를 저장\n",
        "    # 조건에 맞게 데이터쌍을 필터링\n",
        "    # 필터링된 데이터쌍의 80%는 학습용으로, 나머지 20%는 검증용으로 구분\n",
        "    # 전체 데이터쌍의 수, 필터링 후 데이터쌍의 수 그리고 각 언어별 단어(토큰)의 개수를 출력\n",
        "    \n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "\n",
        "    ####################  빈칸  ####################\n",
        "    \n",
        "    # 데이터 shuffle\n",
        "\n",
        "    # 데이터를 8:2로 split\n",
        "\n",
        "    # 각 언어 객체에 문장 추가\n",
        "    \n",
        "    ###############################################\n",
        "\n",
        "    return input_lang, output_lang, train_pairs, valid_pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, train_pairs, valid_pairs = prepareData('eng', 'kor')\n",
        "print(random.choice(train_pairs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNGZrKSHCKKE"
      },
      "source": [
        "인코더 역할의 RNN 생성\n",
        "- torch.nn.Embedding으로 랜덤 임베딩 생성\n",
        "- 인풋의 임베딩을 만들고 배치차원 늘려주기\n",
        "- 임베딩을 gru에 넣고 output과 hidden state 얻기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi9HMKT_LPN2"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        embedding_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.gru = nn.GRU(embedding_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        ####################  빈칸  ####################\n",
        "\n",
        "        # 임베딩 만들고, gru에 입력으로 넣어주고 output 과 hidden 얻기\n",
        "\n",
        "        ###############################################\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vedfHYCXEjqL"
      },
      "source": [
        "디코더 역할의 RNN 생성\n",
        "\n",
        "- torch.nn.Embedding으로 랜덤 임베딩 생성\n",
        "- 인풋의 임베딩을 만들고 배치차원 늘려주기\n",
        "- 임베딩을 gru에 넣고 output과 hidden state 얻기\n",
        "- softmax를 이용하여 전체 단어에서 가장 확률높은 단어 뽑기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhPhntc_LQUG"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        ####################  빈칸  ####################\n",
        "        # GRU의 결과가 linear layer 와 softmax를 거쳐야 한다\n",
        "        # 차원 정의\n",
        "\n",
        "        # layer 정의\n",
        "\n",
        "        # log prob 계산\n",
        "        \n",
        "        ###############################################\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "\n",
        "        ####################  빈칸  ####################\n",
        "\n",
        "        # 임베딩 만들고, gru 입력으로 넣어주고, \n",
        "        # output을 linear 와 softmax의 입력으로 넣어주기        \n",
        "\n",
        "        ###############################################\n",
        "\n",
        "        return output, hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhJleo9-epbA"
      },
      "source": [
        "학습 데이터 준비\n",
        "- 각 문장마다 word2idx를 이용하여 tensor로 변환(벡터화)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zqohs_iHEK3"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "\n",
        "    ####################  빈칸  ####################\n",
        "\n",
        "    # 문장을 띄어쓰기 단위로 토큰화 하고, 해당 단어(토큰)의 id로 변환\n",
        "    # 단, 검증때 사전에 없는 단어가 출현할 수 있는 상황도 처리할 수 있어야 함\n",
        "\n",
        "    ###############################################\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "\n",
        "    ####################  빈칸  ####################\n",
        "\n",
        "    # 실제 데이터 외에 추가로 들어가야할 토큰이 있다!\n",
        "\n",
        "    ###############################################\n",
        "\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9VC8TpcfAFN"
      },
      "source": [
        "모델 학습 코드\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGo9aTzMNVPY"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "# train for each step\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer,\n",
        "          decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    # encoder 초기값 설정, gradient 0 으로 초기화\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    ####################  빈칸  ####################\n",
        "\n",
        "    # encoder 호출\n",
        "\n",
        "        # 각 token에 대해서 hidden과 output을 계산\n",
        "\n",
        "    # decoder의 초기 input과 hidden값 결정\n",
        "\n",
        "    ###############################################\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        ####################  빈칸  ####################\n",
        "\n",
        "        # 디코더의 다음 입력으로 실제값 사용\n",
        "\n",
        "            # decoder로부터 hidden과 output을 계산\n",
        "\n",
        "            # 다음 decoder의 input은 정답으로부터 가져옴\n",
        "\n",
        "        ###############################################\n",
        "\n",
        "    else:\n",
        "        ####################  빈칸  ####################\n",
        "\n",
        "        # 디코더의 다음 입력으로 예측값 사용\n",
        "\n",
        "            # decoder로부터 hidden과 output을 계산\n",
        "\n",
        "            # decoder의 output으로부터 가장 확률이 높은 token을 다음 decoder input으로 사용\n",
        "\n",
        "            # decoder가 EOS token을 생성하면 종료\n",
        "\n",
        "        ###############################################\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzACV8YvgWbQ"
      },
      "source": [
        "시간측정 출력 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNTER1wWNZqg"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n50TJ_uKgawe"
      },
      "source": [
        "학습 진행 과정\n",
        "- 타이머 시작\n",
        "- optimizer, criterion 선언\n",
        "- 전체 학습 데이터에 iteration 수만큼 랜덤하게 데이터 구축"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myAFYp1iNcj7"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "\n",
        "    ####################  빈칸  ####################\n",
        "\n",
        "    # optimizer 선언\n",
        "\n",
        "    # 학습 데이터에서 random하게 선택\n",
        "\n",
        "    # Negative Log Likelihood Loss 정의\n",
        "\n",
        "    ###############################################\n",
        "\n",
        "    # n_iter 횟수만큼 모델 학습 및 로스 출력\n",
        "    for iter in range(1, n_iters + 1):\n",
        "      \n",
        "        ####################  빈칸  ####################\n",
        "\n",
        "        # 학습에 사용할 데이터 input, target 가져오기\n",
        "\n",
        "        # 학습 및 loss 반환\n",
        "        \n",
        "        ###############################################\n",
        "\n",
        "        print_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCAYkwZANkZO"
      },
      "source": [
        "hidden_size = 256\n",
        "\n",
        "####################  빈칸  ####################\n",
        "# encoder, decoder를 선언하고, 7500번의 iteration으로 학습하고 1000번마다 loss 출력\n",
        "\n",
        "# 모델 정의\n",
        "\n",
        "# 학습\n",
        "\n",
        "###############################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDwhujmDi3f_"
      },
      "source": [
        "모델 검증 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfh6b3c8Nhpj"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "\n",
        "        ####################  빈칸  ####################\n",
        "\n",
        "        # input 데이터에서 문장 가져와서 텐서로 변환 (train과 동일)\n",
        "\n",
        "        # 인코더의 초기 hidden state를 결정 (train과 동일)\n",
        "        \n",
        "        ###############################################\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "\n",
        "        ####################  빈칸  ####################\n",
        "\n",
        "        # decoder의 초기 input 과 hidden state 결정 (train과 동일)\n",
        "\n",
        "        ###############################################\n",
        "        \n",
        "        decoded_words = []\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "\n",
        "            ####################  빈칸  ####################\n",
        "\n",
        "            # decoding 멈출 시기 결정, 예측단어 decoded_words에 저장\n",
        "\n",
        "            # EOS_token을 생성하면 종료\n",
        "\n",
        "                # 예측한 단어를 추가\n",
        "\n",
        "            ###############################################\n",
        "            \n",
        "            decoder_input = topi\n",
        "        return decoded_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBzakG5_jix7"
      },
      "source": [
        "검증 데이터쌍에서 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDOlyL1ANiph"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "smoothie = SmoothingFunction().method4\n",
        "eval_pair = [['너 건망증이 좀 있구나.',\"You're quite forgetful.\"],\n",
        "             ['미안한데, 내가 도와줄 수가 없어.',\"I'm sorry, I can't help you.\"],\n",
        "             ['너 떨고 있네.',\"You're shivering.\"],\n",
        "             ['그는 우울하다.',\"He is depressed.\"],\n",
        "             ['난 네 선생이다.',\"I'm your teacher.\"],\n",
        "             ['내 피가 끓고 있었다.\t',\"My blood was boiling.\"]]\n",
        "\n",
        "def evaluatePrint(encoder, decoder):\n",
        "    for pair in eval_pair:\n",
        "        print('원본: ', pair[0])\n",
        "        print('정답 변역: ', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words[:-1])\n",
        "        print('예측 번역: ', output_sentence)\n",
        "        print(\"sentence_bleu: %.3f\" % (sentence_bleu([normalizeString(pair[1]).split()], output_sentence.split(), smoothing_function=smoothie)))\n",
        "        print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chh42lMFs322"
      },
      "source": [
        "전체 검증 데이터의 BLEU score 평균 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0m-U4b3mgG0"
      },
      "source": [
        "def evaluateScore(encoder, decoder):\n",
        "    score = 0.0\n",
        "    for pair in valid_pairs:\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words[:-1])\n",
        "        score += sentence_bleu([normalizeString(pair[1]).split()], output_sentence.split(), smoothing_function=smoothie)\n",
        "    print('Score : ',score/len(valid_pairs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uxAFLElQiVa"
      },
      "source": [
        "evaluatePrint(encoder1, decoder1)\n",
        "evaluateScore(encoder1, decoder1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9gep5hdjrGt"
      },
      "source": [
        "Attention 기법을 적용한 디코더 RNN\n",
        "- dot production 으로 attention score 구하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGComqlqNPB4"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_hiddens, input_length):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        ####################  빈칸  ####################\n",
        "        # gru를 돌고 나온 hidden state로 어텐션 구하기\n",
        "        # attn_score (dot) -> attn distribution -> attn value -> concat (attn value;hidden state)\n",
        "        # 이때, 실제 input 길이를 제외한 나머지 부분은 매우 작은 값(-9e10)으로 마스킹해준 후 softmax를 돌아야함\n",
        "        # concat 된 벡터를 linear 하나 돌아서 output 도출\n",
        "\n",
        "        # output, hidden 계산\n",
        "        \n",
        "        # attn score 구하기 (내적)\n",
        "        \n",
        "        # input을 제외한 부분은 masking\n",
        "\n",
        "        # softmax를 통해 encoder의 hidden embedding들을 weight sum\n",
        "        \n",
        "        # hidden과 encoder hidden에 어텐션을 적용한 결과를 concat하여 최종 단어 예측\n",
        "        \n",
        "        ###############################################\n",
        "\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1sZGd9Enm9h"
      },
      "source": [
        "어텐션 모델 학습 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9eB47UHH7Sq"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "# train for each step\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "\n",
        "    ####################  빈칸  ####################\n",
        "\n",
        "    # 기본 decoder를 사용했을 때와 거의 유사하다.\n",
        "    # decoder 인자에 주의하며, 위에서 작성한 코드를 참고해서 작성\n",
        "    # encoder 의 hiddens state 값들을 모아두어야 함, 미리 transpose 시키기\n",
        "\n",
        "        # 기존의 decoder 학습과 달라진 점: 모든 encdoer의 hidden을 저장 (decoder의 attn에 사용)\n",
        "\n",
        "    # decoder의 input에 encoder의 모든 hidden embedding을 사용\n",
        "    \n",
        "    ###############################################\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOjUT78anqXZ"
      },
      "source": [
        "어텐션 모델 검증 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-yPNX7tIZf9"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "\n",
        "    ####################  빈칸  ####################\n",
        "\n",
        "    # 기본 decoder를 사용했을 때와 거의 유사하다.\n",
        "    # decoder 인자에 주의하며, 위에서 작성한 코드를 참고해서 작성\n",
        "    # encoder 의 hiddens state 값들을 모아두어야 함, 미리 transpose 시키기\n",
        "\n",
        "    # 기존의 decoder 학습과 달라진 점: 모든 encdoer의 hidden을 저장 (decoder의 attn에 사용)\n",
        "\n",
        "        # encoder 호출\n",
        "\n",
        "    ###############################################\n",
        "      \n",
        "        return decoded_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y2dN3AjJJhZ"
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 7500, print_every=1000) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTuJZKtSJE3J"
      },
      "source": [
        "evaluatePrint(encoder1, attn_decoder1)\n",
        "evaluateScore(encoder1, attn_decoder1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ioO7txpOnXmB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}