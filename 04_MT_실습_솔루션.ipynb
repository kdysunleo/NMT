{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/kdysunleo/NMT/main/eng-kor.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3LMdVcZTseOK",
        "outputId": "f996d97f-f6e7-4161-8598-5843881cf783"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-01-11 12:17:04--  https://raw.githubusercontent.com/kdysunleo/NMT/main/eng-kor.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 534332 (522K) [text/plain]\n",
            "Saving to: ‘eng-kor.txt.1’\n",
            "\n",
            "\reng-kor.txt.1         0%[                    ]       0  --.-KB/s               \reng-kor.txt.1       100%[===================>] 521.81K  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-01-11 12:17:04 (14.2 MB/s) - ‘eng-kor.txt.1’ saved [534332/534332]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PVbkYvQ47ayQ"
      },
      "source": [
        "**Sequence to Sequence 네트워크와 Attention을 이용한 Machine Translation 모델 구현**\n",
        "\n",
        "- 한국어를 영어로 번역하도록 모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l2cxUfvGzpcZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8c35cb5-34da-4be0-c152-ac7ad9f8c527"
      },
      "source": [
        "import string\n",
        "import re\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch import optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "seed = 21\n",
        "random.seed(seed)\n",
        "torch.manual_seed(seed)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f09e19148d0>"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xQ4C8wO5QcV"
      },
      "source": [
        "언어별 정보 저장을 위한 클래스 생성\n",
        "\n",
        "- 단어->아이디, 아이디->단어 사전 생성 (word2idx)\n",
        "- addSentence 함수는 문장을 띄어쓰기를 기준으로 토큰화\n",
        "- addWord 함수\n",
        "    - word2idx, idx2word 만들기\n",
        "    - 단어별 출현 횟수 구하기\n",
        "    - 전체 단어의 개수 구하기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A5V2jJCG0Qot"
      },
      "source": [
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "UNK_token = 2\n",
        "\n",
        "class Lang:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        ## 단어->아이디, 단어별 개수 사전 선언\n",
        "\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "\n",
        "        self.index2word = {0: \"SOS\", 1: \"EOS\", 2:\"UNK\"}\n",
        "        self.n_words = 3  # Count SOS and EOS and UNK\n",
        "\n",
        "    def addSentence(self, sentence):\n",
        "        # sentence를 띄어쓰기 단위로 토큰화 하고, 토큰은 addWord 함수를 이용해서 정보 저장\n",
        "\n",
        "        for word in sentence.split(' '):\n",
        "            self.addWord(word)\n",
        "\n",
        "    def addWord(self, word):\n",
        "        # word2idx, word2count, idx2word, nwords 정보 저장\n",
        "\n",
        "        ####################  빈칸  ####################\n",
        "        \n",
        "        # 단어를 Dictionary에 추가 / 업데이트\n",
        "        if word not in self.word2index: \n",
        "            self.word2index[word] = self.n_words # word에 idx를 mapping\n",
        "            self.word2count[word] = 1 # 단어의 등장 횟수 = 1\n",
        "            self.index2word[self.n_words] = word # idx에 word를 mapping\n",
        "            self.n_words += 1 # \n",
        "        else:\n",
        "            self.word2count[word] += 1 # 단어의 등장 횟수 + 1\n",
        "            \n",
        "        ###############################################"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kp3YN2_M7FSy"
      },
      "source": [
        "- 병렬 코퍼스 준비 : 영어-한국어\n",
        "- 영어는 normalize를 진행\n",
        "- readLangs 함수\n",
        "    - 파일을 읽고 쌍으로 분리\n",
        "    - Lang class를 이용하여 각 언어 정보 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A1N6GRxS5d5-"
      },
      "source": [
        "def normalizeString(s):\n",
        "    s = s.lower().strip()\n",
        "    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n",
        "    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n",
        "    return s\n",
        "\n",
        "def readLangs(lang1, lang2):\n",
        "    print(\"Reading lines...\")\n",
        "\n",
        "    # git clone을 통해 데이터를 다운로드 받는 경우, 파일 경로 수정 필요\n",
        "    f = open('%s-%s.txt' % (lang1, lang2), encoding='utf-8')\n",
        "    lines = f.readlines()\n",
        "    \n",
        "    pairs = []\n",
        "    for l in lines:\n",
        "        line = l.split(\"\\t\")\n",
        "        # 한국어, 영어\n",
        "        pair = [line[1], normalizeString(line[0])]\n",
        "        pairs.append(pair)\n",
        "\n",
        "    input_lang = Lang(lang2)\n",
        "    output_lang = Lang(lang1)\n",
        "    f.close()\n",
        "    return input_lang, output_lang, pairs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "voejo56ravk2"
      },
      "source": [
        "- MAX_LENGTH : 학습에 사용할 최대문장길이\n",
        "- eng_prefixes : 해당 단어들로 시작하는 문장만 번역에 포함\n",
        "\n",
        "- filterPair : 위 조건에 만족하는 pair만 반환\n",
        "- filterPairs : 필터링된 pair만 저장해서 반환"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jqvzt5j5on-"
      },
      "source": [
        "MAX_LENGTH = 30\n",
        "eng_prefixes = (\n",
        "    \"i am \", \"i m \",\n",
        "    \"he is\", \"he s \",\n",
        "    \"she is\", \"she s \",\n",
        "    \"you are\", \"you re \",\n",
        "    \"we are\", \"we re \",\n",
        "    \"they are\", \"they re \"\n",
        ")\n",
        "\n",
        "def filterPair(p):\n",
        "    ####################  빈칸  ####################\n",
        "\n",
        "    # 영어는 해당하는 구문으로 시작하고 영어와 한국어 둘다 max_length보다 작은 데이터일 경우에만 True 반환\n",
        "    return len(p[0].split(' ')) < MAX_LENGTH and len(p[1].split(' ')) < MAX_LENGTH and p[1].startswith(eng_prefixes)\n",
        "    \n",
        "    ###############################################\n",
        "\n",
        "def filterPairs(pairs):\n",
        "    # filterPair가 True인 데이터만 저장후 반환\n",
        "    return [pair for pair in pairs if filterPair(pair)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00R1vJMkbeg5"
      },
      "source": [
        "데이터 준비 및 결과 확인\n",
        "- **학습과 검증에 필요한 데이터쌍을 구분**\n",
        "- 이전 단계에서 만든 클래스와 함수를 통해 결과 확인\n",
        "- 전체 문장 개수(pair 개수)\n",
        "- 필터링 후 문장개수(pair 개수)\n",
        "- 각 언어별로, 사전에 등록된 단어 개수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7b318yrC5vEL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2921daf7-0404-4955-e47f-6677c223c6f3"
      },
      "source": [
        "def prepareData(lang1, lang2):\n",
        "\n",
        "    # 이전에 만든 클래스를 활용해서 영어 한국어의 정보를 저장\n",
        "    # 조건에 맞게 데이터쌍을 필터링\n",
        "    # 필터링된 데이터쌍의 80%는 학습용으로, 나머지 20%는 검증용으로 구분\n",
        "    # 전체 데이터쌍의 수, 필터링 후 데이터쌍의 수 그리고 각 언어별 단어(토큰)의 개수를 출력\n",
        "    \n",
        "    input_lang, output_lang, pairs = readLangs(lang1, lang2)\n",
        "    print(\"Read %s sentence pairs\" % len(pairs))\n",
        "    pairs = filterPairs(pairs)\n",
        "    print(\"Trimmed to %s sentence pairs\" % len(pairs))\n",
        "\n",
        "\n",
        "    ####################  빈칸  ####################\n",
        "    \n",
        "    # 데이터 shuffle\n",
        "    random.shuffle(pairs)\n",
        "\n",
        "    # 데이터를 8:2로 split\n",
        "    train_size = int(len(pairs)*0.8) \n",
        "    train_pairs = pairs[:train_size]\n",
        "    valid_pairs = pairs[train_size:]\n",
        "\n",
        "    # 각 언어 객체에 문장 추가\n",
        "    for pair in train_pairs:\n",
        "        input_lang.addSentence(pair[0])\n",
        "        output_lang.addSentence(pair[1])\n",
        "\n",
        "    print(\"입력 언어 : \",input_lang.name, input_lang.n_words)\n",
        "    print(\"출력 언어 : \",output_lang.name, output_lang.n_words)\n",
        "    \n",
        "    ###############################################\n",
        "\n",
        "    return input_lang, output_lang, train_pairs, valid_pairs\n",
        "\n",
        "\n",
        "input_lang, output_lang, train_pairs, valid_pairs = prepareData('eng', 'kor')\n",
        "print(random.choice(train_pairs))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading lines...\n",
            "Read 3648 sentence pairs\n",
            "Trimmed to 223 sentence pairs\n",
            "입력 언어 :  kor 439\n",
            "출력 언어 :  eng 308\n",
            "['나는 내년에 호주에 간다.', 'i m going to australia next year .']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QNGZrKSHCKKE"
      },
      "source": [
        "인코더 역할의 RNN 생성\n",
        "- torch.nn.Embedding으로 랜덤 임베딩 생성\n",
        "- 인풋의 임베딩을 만들고 배치차원 늘려주기\n",
        "- 임베딩을 gru에 넣고 output과 hidden state 얻기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yi9HMKT_LPN2"
      },
      "source": [
        "class EncoderRNN(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size):\n",
        "        super(EncoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        embedding_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, embedding_size)\n",
        "        self.gru = nn.GRU(embedding_size, hidden_size)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        ####################  빈칸  ####################\n",
        "\n",
        "        # 임베딩 만들고, gru에 입력으로 넣어주고 output 과 hidden 얻기\n",
        "        embedded = self.embedding(input).view(1, 1, -1) # word idx으로부터 word embedding 만들기\n",
        "        output, hidden = self.gru(embedded, hidden) # gru\n",
        "\n",
        "        ###############################################\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vedfHYCXEjqL"
      },
      "source": [
        "디코더 역할의 RNN 생성\n",
        "\n",
        "- torch.nn.Embedding으로 랜덤 임베딩 생성\n",
        "- 인풋의 임베딩을 만들고 배치차원 늘려주기\n",
        "- 임베딩을 gru에 넣고 output과 hidden state 얻기\n",
        "- softmax를 이용하여 전체 단어에서 가장 확률높은 단어 뽑기\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NhPhntc_LQUG"
      },
      "source": [
        "class DecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size):\n",
        "        super(DecoderRNN, self).__init__()\n",
        "\n",
        "        ####################  빈칸  ####################\n",
        "        # GRU의 결과가 linear layer 와 softmax를 거쳐야 한다\n",
        "        # 차원 정의\n",
        "        self.hidden_size = hidden_size\n",
        "        embedding_size = hidden_size\n",
        "        # layer 정의\n",
        "        self.embedding = nn.Embedding(output_size, embedding_size)\n",
        "        self.gru = nn.GRU(embedding_size, hidden_size)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        # log prob 계산\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "        \n",
        "        ###############################################\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "\n",
        "        ####################  빈칸  ####################\n",
        "        # 임베딩 만들고, gru 입력으로 넣어주고, \n",
        "        # output을 linear 와 softmax의 입력으로 넣어주기\n",
        "\n",
        "        # batch_size = 1\n",
        "        # input (단어의 index) : batch_size X 1\n",
        "        # self.embedding(input) : batch_size X embedding_size    (embedding_size = hidden_size)\n",
        "        # output = self.embedding(input).view(1, 1, -1) : batch_size X seq_len x embedding_size (batch_size = 1)\n",
        "        # hidden (이전 step의 hidden vector) : batch_size X hidden_size\n",
        "        # output, hidden = self.gru(output, hidden) : 각각 batch_size X hidden_size\n",
        "        # word embedding (1 X 1 X Embedding_size)\n",
        "\n",
        "        output = self.embedding(input).view(1, 1, -1) # word idx으로부터 word embedding 만들기\n",
        "        output, hidden = self.gru(output, hidden) # gru, encoder의 hidden을 init\n",
        "\n",
        "        # attention 연산이 들어갈 부분\n",
        "        \n",
        "        output = self.log_softmax(self.out(output[0])) # decoder gru의 output으로 log prob을 계산\n",
        "\n",
        "        ###############################################\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "#    def initHidden(self):\n",
        "#        return torch.zeros(1, 1, self.hidden_size, device=device)\n",
        "        "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhJleo9-epbA"
      },
      "source": [
        "학습 데이터 준비\n",
        "- 각 문장마다 word2idx를 이용하여 tensor로 변환(벡터화)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-zqohs_iHEK3"
      },
      "source": [
        "def indexesFromSentence(lang, sentence):\n",
        "    ####################  빈칸  ####################\n",
        "\n",
        "    # 문장을 띄어쓰기 단위로 토큰화 하고, 해당 단어(토큰)의 id로 변환\n",
        "    # 단, 검증때 사전에 없는 단어가 출현할 수 있는 상황도 처리할 수 있어야 함\n",
        "    result = []\n",
        "    for word in sentence.split(' '):\n",
        "        if word in lang.word2index:\n",
        "            result.append(lang.word2index[word]) # 각 단어에 해당하는 idx 추가\n",
        "        else:\n",
        "            result.append(UNK_token) # 해당 단어가 존재하지 않는 경우 UNK token 추가\n",
        "    return result\n",
        "\n",
        "    ###############################################\n",
        "\n",
        "\n",
        "def tensorFromSentence(lang, sentence):\n",
        "    indexes = indexesFromSentence(lang, sentence)\n",
        "\n",
        "    ####################  빈칸  ####################\n",
        "\n",
        "    # 실제 데이터 외에 추가로 들어가야할 토큰이 있다!\n",
        "    indexes.append(EOS_token) # EOS token 추가\n",
        "\n",
        "    ###############################################\n",
        "\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "def tensorsFromPair(pair):\n",
        "    input_tensor = tensorFromSentence(input_lang, pair[0])\n",
        "    target_tensor = tensorFromSentence(output_lang, pair[1])\n",
        "    return (input_tensor, target_tensor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9VC8TpcfAFN"
      },
      "source": [
        "모델 학습 코드\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rGo9aTzMNVPY"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "# train for each step\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer,\n",
        "          decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "    # encoder 초기값 설정, gradient 0 으로 초기화\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    ####################  빈칸  ####################\n",
        "    # encoder 호출\n",
        "    for ei in range(input_length):\n",
        "        # 각 token에 대해서 hidden과 output을 계산\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        \n",
        "    # decoder의 초기 input과 hidden값 결정\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device) # SOS 토큰\n",
        "    decoder_hidden = encoder_hidden # encoder의 hidden을 decoder의 초기 hidden으로 사용\n",
        "    ###############################################\n",
        "\n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    if use_teacher_forcing:\n",
        "        ####################  빈칸  ####################\n",
        "        # 디코더의 다음 입력으로 실제값 사용\n",
        "        for di in range(target_length):\n",
        "            # decoder로부터 hidden과 output을 계산\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            loss += criterion(decoder_output, target_tensor[di]) # loss 계산\n",
        "            # 다음 decoder의 input은 정답으로부터 가져옴\n",
        "            decoder_input = target_tensor[di]\n",
        "        ###############################################\n",
        "\n",
        "    else:\n",
        "        ####################  빈칸  ####################\n",
        "\n",
        "        # 디코더의 다음 입력으로 예측값 사용\n",
        "        for di in range(target_length):\n",
        "            # decoder로부터 hidden과 output을 계산\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            \n",
        "            # decoder의 output으로부터 가장 확률이 높은 token을 다음 decoder input으로 사용\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.detach()\n",
        "            \n",
        "            loss += criterion(decoder_output, target_tensor[di]) # loss 계산\n",
        "            # decoder가 EOS token을 생성하면 종료\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "        ###############################################\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AzACV8YvgWbQ"
      },
      "source": [
        "시간측정 출력 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nNTER1wWNZqg"
      },
      "source": [
        "import time\n",
        "import math\n",
        "\n",
        "def asMinutes(s):\n",
        "    m = math.floor(s / 60)\n",
        "    s -= m * 60\n",
        "    return '%dm %ds' % (m, s)\n",
        "\n",
        "def timeSince(since, percent):\n",
        "    now = time.time()\n",
        "    s = now - since\n",
        "    es = s / (percent)\n",
        "    rs = es - s\n",
        "    return '%s (- %s)' % (asMinutes(s), asMinutes(rs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n50TJ_uKgawe"
      },
      "source": [
        "학습 진행 과정\n",
        "- 타이머 시작\n",
        "- optimizer, criterion 선언\n",
        "- 전체 학습 데이터에 iteration 수만큼 랜덤하게 데이터 구축"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myAFYp1iNcj7"
      },
      "source": [
        "def trainIters(encoder, decoder, n_iters, print_every=1000, learning_rate=0.01):\n",
        "    start = time.time()\n",
        "    print_loss_total = 0  # Reset every print_every\n",
        "\n",
        "    ####################  빈칸  ####################\n",
        "    # optimizer 선언, 데이터 준비, loss함수 선언  \n",
        "    # optimizer 선언\n",
        "    encoder_optimizer = optim.SGD(encoder.parameters(), lr=learning_rate)\n",
        "    decoder_optimizer = optim.SGD(decoder.parameters(), lr=learning_rate) # Adam / AdamW\n",
        "    # 학습 데이터에서 random하게 선택\n",
        "    training_pairs = [tensorsFromPair(random.choice(train_pairs))\n",
        "                      for i in range(n_iters)]\n",
        "    # Negative Log Likelihood Loss 정의\n",
        "    criterion = nn.NLLLoss()\n",
        "    ###############################################\n",
        "    # n_iter 횟수만큼 모델 학습 및 로스 출력\n",
        "    for iter in range(1, n_iters + 1):\n",
        "      \n",
        "        ####################  빈칸  ####################\n",
        "        # 학습에 사용할 데이터 input, target 가져오기\n",
        "        training_pair = training_pairs[iter - 1]\n",
        "        input_tensor = training_pair[0]\n",
        "        target_tensor = training_pair[1]\n",
        "        # 학습 및 loss 반환\n",
        "        loss = train(input_tensor, target_tensor, encoder,\n",
        "                     decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "        \n",
        "        ###############################################\n",
        "\n",
        "        print_loss_total += loss\n",
        "\n",
        "        if iter % print_every == 0:\n",
        "            print_loss_avg = print_loss_total / print_every\n",
        "            print_loss_total = 0\n",
        "            print('%s (%d %d%%) %.4f' % (timeSince(start, iter / n_iters),\n",
        "                                         iter, iter / n_iters * 100, print_loss_avg))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCAYkwZANkZO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae0b9007-565d-4742-ccf3-fe5996934798"
      },
      "source": [
        "hidden_size = 256\n",
        "\n",
        "####################  빈칸  ####################\n",
        "# encoder, decoder를 선언하고, 7500번의 iteration으로 학습하고 1000번마다 loss 출력\n",
        "# 모델 정의\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "decoder1 = DecoderRNN(hidden_size, output_lang.n_words).to(device)\n",
        "\n",
        "# 학습\n",
        "trainIters(encoder1, decoder1, 7500, print_every=1000)\n",
        "###############################################"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0m 11s (- 1m 12s) (1000 13%) 2.6594\n",
            "0m 18s (- 0m 52s) (2000 26%) 1.9629\n",
            "0m 27s (- 0m 40s) (3000 40%) 1.3645\n",
            "0m 35s (- 0m 30s) (4000 53%) 0.7722\n",
            "0m 43s (- 0m 21s) (5000 66%) 0.3931\n",
            "0m 51s (- 0m 12s) (6000 80%) 0.2011\n",
            "0m 59s (- 0m 4s) (7000 93%) 0.1191\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BDwhujmDi3f_"
      },
      "source": [
        "모델 검증 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gfh6b3c8Nhpj"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    with torch.no_grad():\n",
        "\n",
        "        ####################  빈칸  ####################\n",
        "\n",
        "        # input 데이터에서 문장 가져와서 텐서로 변환 (train과 동일)\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "\n",
        "        # 인코더의 초기 hidden state를 결정 (train과 동일)\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "        \n",
        "        ###############################################\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei],\n",
        "                                                     encoder_hidden)\n",
        "\n",
        "        ####################  빈칸  ####################\n",
        "\n",
        "        # decoder의 초기 input 과 hidden state 결정 (train과 동일)\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        ###############################################\n",
        "        \n",
        "        decoded_words = []\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "\n",
        "            ####################  빈칸  ####################\n",
        "\n",
        "            # decoding 멈출 시기 결정, 예측단어 decoded_words에 저장\n",
        "\n",
        "            # EOS_token을 생성하면 종료\n",
        "            # topi = tensor([3])\n",
        "            # topi.item() = 3\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                # 예측한 단어를 추가\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "\n",
        "            ###############################################\n",
        "            \n",
        "            decoder_input = topi\n",
        "        return decoded_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBzakG5_jix7"
      },
      "source": [
        "검증 데이터쌍에서 결과 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DDOlyL1ANiph"
      },
      "source": [
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "smoothie = SmoothingFunction().method4 # 짧은 문장의 score가 너무 높아지는 것을 방지\n",
        "\n",
        "# BLEU score (n-gram)\n",
        "# 예측 : A B C D E\n",
        "# 정답 : A B C E D\n",
        "# 1-gram A / B / C / D / E\n",
        "# 2-gram (A, B) / (B, C)\n",
        "# 3-gram\n",
        "# 4-gram\n",
        "\n",
        "eval_pair = [['너 건망증이 좀 있구나.',\"You're quite forgetful.\"],\n",
        "             ['미안한데, 내가 도와줄 수가 없어.',\"I'm sorry, I can't help you.\"],\n",
        "             ['너 떨고 있네.',\"You're shivering.\"],\n",
        "             ['그는 우울하다.',\"He is depressed.\"],\n",
        "             ['난 네 선생이다.',\"I'm your teacher.\"],\n",
        "             ['내 피가 끓고 있었다.\t',\"My blood was boiling.\"]]\n",
        "\n",
        "def evaluatePrint(encoder, decoder):\n",
        "    for pair in eval_pair:\n",
        "        print('원본: ', pair[0])\n",
        "        print('정답 변역: ', pair[1])\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words[:-1])\n",
        "        print('예측 번역: ', output_sentence)\n",
        "        print(\"sentence_bleu: %.3f\" % (sentence_bleu([normalizeString(pair[1]).split()], output_sentence.split(), smoothing_function=smoothie)))\n",
        "        print('')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "chh42lMFs322"
      },
      "source": [
        "전체 검증 데이터의 BLEU score 평균 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0m-U4b3mgG0"
      },
      "source": [
        "def evaluateScore(encoder, decoder):\n",
        "    score = 0.0\n",
        "    for pair in valid_pairs:\n",
        "        output_words = evaluate(encoder, decoder, pair[0])\n",
        "        output_sentence = ' '.join(output_words[:-1])\n",
        "        score += sentence_bleu([normalizeString(pair[1]).split()], output_sentence.split(), smoothing_function=smoothie)\n",
        "    print('Score : ',score/len(valid_pairs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uxAFLElQiVa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1ecbb9ed-0c0e-4422-e162-3b006c75f1d1"
      },
      "source": [
        "evaluatePrint(encoder1, decoder1)\n",
        "evaluateScore(encoder1, decoder1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원본:  너 건망증이 좀 있구나.\n",
            "정답 변역:  You're quite forgetful.\n",
            "예측 번역:  you re quite forgetful .\n",
            "sentence_bleu: 1.000\n",
            "\n",
            "원본:  미안한데, 내가 도와줄 수가 없어.\n",
            "정답 변역:  I'm sorry, I can't help you.\n",
            "예측 번역:  i m sorry i can t help you .\n",
            "sentence_bleu: 1.000\n",
            "\n",
            "원본:  너 떨고 있네.\n",
            "정답 변역:  You're shivering.\n",
            "예측 번역:  i m sorry .\n",
            "sentence_bleu: 0.061\n",
            "\n",
            "원본:  그는 우울하다.\n",
            "정답 변역:  He is depressed.\n",
            "예측 번역:  she s depressed .\n",
            "sentence_bleu: 0.168\n",
            "\n",
            "원본:  난 네 선생이다.\n",
            "정답 변역:  I'm your teacher.\n",
            "예측 번역:  i m sorry .\n",
            "sentence_bleu: 0.145\n",
            "\n",
            "원본:  내 피가 끓고 있었다.\t\n",
            "정답 변역:  My blood was boiling.\n",
            "예측 번역:  i m sorry but there s no other way .\n",
            "sentence_bleu: 0.023\n",
            "\n",
            "Score :  0.13787596384361508\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w9gep5hdjrGt"
      },
      "source": [
        "Attention 기법을 적용한 디코더 RNN\n",
        "- dot production 으로 attention score 구하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGComqlqNPB4"
      },
      "source": [
        "class AttnDecoderRNN(nn.Module):\n",
        "    def __init__(self, hidden_size, output_size, dropout_p=0.1, max_length=MAX_LENGTH):\n",
        "        super(AttnDecoderRNN, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.output_size = output_size\n",
        "        self.dropout_p = dropout_p\n",
        "        self.max_length = max_length\n",
        "\n",
        "        self.embedding = nn.Embedding(self.output_size, self.hidden_size)\n",
        "        self.dropout = nn.Dropout(self.dropout_p)\n",
        "        self.gru = nn.GRU(self.hidden_size, self.hidden_size)\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.out = nn.Linear(self.hidden_size*2, self.output_size)\n",
        "        self.log_softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden, encoder_hiddens, input_length):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        embedded = self.dropout(embedded)\n",
        "\n",
        "        ####################  빈칸  ####################\n",
        "        # gru를 돌고 나온 hidden state로 어텐션 구하기\n",
        "        # attn_score (dot) -> attn distribution -> attn value -> concat (attn value;hidden state)\n",
        "        # 이때, 실제 input 길이를 제외한 나머지 부분은 매우 작은 값(-9e10)으로 마스킹해준 후 softmax를 돌아야함\n",
        "        # concat 된 벡터를 linear 하나 돌아서 output 도출\n",
        "        # output, hidden 계산\n",
        "        output, hidden = self.gru(embedded, hidden)\n",
        "        # output, hidden : batch_size(1) x 1 x hidden_size\n",
        "        # attn score 구하기 (내적)\n",
        "        attn_score = torch.mm(hidden.squeeze(0), encoder_hiddens)\n",
        "        # hidden.squeeze(0) : 1 x hidden_size     ex) [[1, 2]] : 1 X 2 -> [1, 2] : 2\n",
        "        # encoder_hiddens : hidden_size x input_length\n",
        "        # attn_score : (1 x hidden_size) x (hidden_size x input_length) => 1 x input_length\n",
        "        # input을 제외한 부분은 masking\n",
        "        mask = torch.full((1,self.max_length-input_length), -9e10)\n",
        "        attn_score[0][input_length:]=mask\n",
        "        # softmax를 통해 encoder의 hidden embedding들을 weight sum\n",
        "        attn_dist = self.softmax(attn_score)\n",
        "        # 분포가 확률분포로 변함, 차원 변환 X\n",
        "        # attn_dist : 1 x input_length\n",
        "        attn_value = torch.mm(attn_dist, torch.transpose(encoder_hiddens,0,1))\n",
        "        # torch.transpose(encoder_hiddens,0,1) : input_length X hidden_size     0, 1차원을 변환\n",
        "        # attn_value = (1 x input_length) X (input_length x hidden_size) => 1 x hidden_size\n",
        "        # hidden과 encoder hidden에 어텐션을 적용한 결과를 concat하여 최종 단어 예측\n",
        "        output =  torch.cat((hidden.squeeze(0), attn_value), dim=1)\n",
        "        # hidden.squeeze(0) : 1 x hidden_size\n",
        "        # torch.cat(([1, 2], [3, 4])) => [1, 2, 3, 4]\n",
        "        # torch.cat((hidden.squeeze(0), attn_value), dim=1) => 1 x 2 hidden_size\n",
        "        output = self.log_softmax(self.out(output))\n",
        "        ###############################################\n",
        "\n",
        "\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(1, 1, self.hidden_size, device=device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1sZGd9Enm9h"
      },
      "source": [
        "어텐션 모델 학습 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B9eB47UHH7Sq"
      },
      "source": [
        "teacher_forcing_ratio = 0.5\n",
        "\n",
        "# train for each step\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=MAX_LENGTH):\n",
        "\n",
        "    ####################  빈칸  ####################\n",
        "\n",
        "    # 기본 decoder를 사용했을 때와 거의 유사하다.\n",
        "    # decoder 인자에 주의하며, 위에서 작성한 코드를 참고해서 작성\n",
        "    # encoder 의 hiddens state 값들을 모아두어야 함, 미리 transpose 시키기\n",
        "\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    encoder_hiddens = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "    # 30 X 256 차원 0 텐서\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    # encoder 호출\n",
        "    for ei in range(input_length):\n",
        "    \n",
        "        # 기존의 decoder 학습과 달라진 점: 모든 encdoer의 hidden을 저장 (decoder의 attn에 사용)\n",
        "        encoder_output, encoder_hidden = encoder(\n",
        "            input_tensor[ei], encoder_hidden)\n",
        "        encoder_hiddens[ei] = encoder_hidden.view(-1, encoder.hidden_size) # 1 X 256 차원\n",
        "\n",
        "    encoder_hiddens = torch.transpose(encoder_hiddens, 0, 1) # 256 x 30 차원\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "    decoder_hidden = encoder_hidden\n",
        "    \n",
        "    use_teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
        "\n",
        "    # decoder의 input에 encoder의 모든 hidden embedding을 사용\n",
        "    if use_teacher_forcing:\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_hiddens, input_length)\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            decoder_input = target_tensor[di]  # Teacher forcing\n",
        "\n",
        "    else:\n",
        "        for di in range(target_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_hiddens, input_length)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "            decoder_input = topi.detach()\n",
        "            loss += criterion(decoder_output, target_tensor[di])\n",
        "            if decoder_input.item() == EOS_token:\n",
        "                break\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "    \n",
        "    ###############################################\n",
        "\n",
        "    return loss.item() / target_length"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jOjUT78anqXZ"
      },
      "source": [
        "어텐션 모델 검증 코드"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J-yPNX7tIZf9"
      },
      "source": [
        "def evaluate(encoder, decoder, sentence, max_length=MAX_LENGTH):\n",
        "    ####################  빈칸  ####################\n",
        "\n",
        "    # 기본 decoder를 사용했을 때와 거의 유사하다.\n",
        "    # decoder 인자에 주의하며, 위에서 작성한 코드를 참고해서 작성\n",
        "    # encoder 의 hiddens state 값들을 모아두어야 함, 미리 transpose 시키기\n",
        "\n",
        "    # 기존의 decoder 학습과 달라진 점: 모든 encdoer의 hidden을 저장 (decoder의 attn에 사용)\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromSentence(input_lang, sentence)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        encoder_hiddens = torch.zeros(max_length, encoder.hidden_size, device=device)\n",
        "\n",
        "        # encoder 호출\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(\n",
        "                input_tensor[ei], encoder_hidden)\n",
        "            encoder_hiddens[ei] = encoder_hidden.view(-1, encoder.hidden_size)\n",
        "        encoder_hiddens = torch.transpose(encoder_hiddens, 0, 1)\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "        decoded_words = []\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(\n",
        "                decoder_input, decoder_hidden, encoder_hiddens, input_length)\n",
        "            topv, topi = decoder_output.data.topk(1)\n",
        "            if topi.item() == EOS_token:\n",
        "                decoded_words.append('<EOS>')\n",
        "                break\n",
        "            else:\n",
        "                decoded_words.append(output_lang.index2word[topi.item()])\n",
        "            decoder_input = topi\n",
        "\n",
        "      ###############################################\n",
        "      \n",
        "        return decoded_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6y2dN3AjJJhZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c597fcb4-3948-4d25-d7ba-c7e4c270eea7"
      },
      "source": [
        "hidden_size = 256\n",
        "encoder1 = EncoderRNN(input_lang.n_words, hidden_size).to(device)\n",
        "attn_decoder1 = AttnDecoderRNN(hidden_size, output_lang.n_words, dropout_p=0.1).to(device)\n",
        "\n",
        "trainIters(encoder1, attn_decoder1, 7500, print_every=1000) "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0m 12s (- 1m 19s) (1000 13%) 2.2779\n",
            "0m 25s (- 1m 9s) (2000 26%) 0.9982\n",
            "0m 38s (- 0m 57s) (3000 40%) 0.3078\n",
            "0m 50s (- 0m 44s) (4000 53%) 0.1065\n",
            "1m 3s (- 0m 31s) (5000 66%) 0.0648\n",
            "1m 16s (- 0m 19s) (6000 80%) 0.0370\n",
            "1m 29s (- 0m 6s) (7000 93%) 0.0367\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DTuJZKtSJE3J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17988fc9-711d-4c54-f36c-49a5287bdcd2"
      },
      "source": [
        "evaluatePrint(encoder1, attn_decoder1)\n",
        "evaluateScore(encoder1, attn_decoder1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "원본:  너 건망증이 좀 있구나.\n",
            "정답 변역:  You're quite forgetful.\n",
            "예측 번역:  you re quite forgetful .\n",
            "sentence_bleu: 1.000\n",
            "\n",
            "원본:  미안한데, 내가 도와줄 수가 없어.\n",
            "정답 변역:  I'm sorry, I can't help you.\n",
            "예측 번역:  i m sorry i can t help you .\n",
            "sentence_bleu: 1.000\n",
            "\n",
            "원본:  너 떨고 있네.\n",
            "정답 변역:  You're shivering.\n",
            "예측 번역:  i m sorry but i don t you ?\n",
            "sentence_bleu: 0.026\n",
            "\n",
            "원본:  그는 우울하다.\n",
            "정답 변역:  He is depressed.\n",
            "예측 번역:  she s depressed .\n",
            "sentence_bleu: 0.168\n",
            "\n",
            "원본:  난 네 선생이다.\n",
            "정답 변역:  I'm your teacher.\n",
            "예측 번역:  i m a sorry i kept you .\n",
            "sentence_bleu: 0.079\n",
            "\n",
            "원본:  내 피가 끓고 있었다.\t\n",
            "정답 변역:  My blood was boiling.\n",
            "예측 번역:  i m sorry but i don t you ?\n",
            "sentence_bleu: 0.000\n",
            "\n",
            "Score :  0.19931846065660075\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ioO7txpOnXmB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}